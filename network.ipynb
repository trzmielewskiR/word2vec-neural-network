{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "33995bc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\eryoo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gzip\n",
    "\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6921537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    labels = []\n",
    "    texts = []\n",
    "\n",
    "    with gzip.open(file_path, 'rt', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) < 2:\n",
    "                print(f\"Skipping malformed line {line_num}: {line}\")\n",
    "                continue\n",
    "            label = parts[0]\n",
    "            text = '\\t'.join(parts[1:])\n",
    "            labels.append(label)\n",
    "            texts.append(text)\n",
    "\n",
    "    return pd.DataFrame({'label': labels, 'text': texts})\n",
    "\n",
    "def read_tsv(file_path):\n",
    "    lines = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            lines.append(line.rstrip('\\n'))\n",
    "    return pd.DataFrame(lines, columns=['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1d06a5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data('train/train.tsv.gz')\n",
    "dev_df = pd.read_csv('dev-0/in.tsv', sep='\\t', header=None, names=['text'])\n",
    "dev_labels = pd.read_csv('dev-0/expected.tsv', sep='\\t', header=None, names=['label'])\n",
    "test_df = read_tsv('test-A/in.tsv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "df2986aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text.lower())\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "28df79d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['tokens'] = train_df['text'].apply(preprocess)\n",
    "dev_df['tokens'] = dev_df['text'].apply(preprocess)\n",
    "test_df['tokens'] = test_df['text'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "61d0f2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download polish fasttext embedding: https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.pl.300.vec.gz\n",
    "# and put it directly in the current directory (warning: after unpacking it will be 4,5 GB)\n",
    "fasttext_path = 'cc.pl.300.vec'\n",
    "word2vec = KeyedVectors.load_word2vec_format(fasttext_path, binary=False)\n",
    "vector_size = word2vec.vector_size\n",
    "\n",
    "def vectorize(tokens, model, vector_size):\n",
    "    vecs = [model[word] for word in tokens if word in model]\n",
    "    if not vecs:\n",
    "        return np.zeros(vector_size)\n",
    "    return np.mean(vecs, axis=0)\n",
    "\n",
    "train_vectors = np.array([vectorize(tokens, word2vec, vector_size) for tokens in train_df['tokens']])\n",
    "dev_vectors = np.array([vectorize(tokens, word2vec, vector_size) for tokens in dev_df['tokens']])\n",
    "test_vectors = np.array([vectorize(tokens, word2vec, vector_size) for tokens in test_df['tokens']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "c306a075",
   "metadata": {},
   "outputs": [],
   "source": [
    "dev_labels['label'] = dev_labels['label'].astype(str)\n",
    "label_encoder = LabelEncoder()\n",
    "y_train = label_encoder.fit_transform(train_df['label'])\n",
    "y_dev = label_encoder.transform(dev_labels['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069dc7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8967 - loss: 0.2528 - val_accuracy: 0.9600 - val_loss: 0.1099\n",
      "Epoch 2/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9496 - loss: 0.1393 - val_accuracy: 0.9356 - val_loss: 0.1694\n",
      "Epoch 3/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9527 - loss: 0.1324 - val_accuracy: 0.9666 - val_loss: 0.0923\n",
      "Epoch 4/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9561 - loss: 0.1240 - val_accuracy: 0.9666 - val_loss: 0.0929\n",
      "Epoch 5/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9571 - loss: 0.1191 - val_accuracy: 0.9685 - val_loss: 0.0867\n",
      "Epoch 6/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9584 - loss: 0.1150 - val_accuracy: 0.9710 - val_loss: 0.0805\n",
      "Epoch 7/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9626 - loss: 0.1081 - val_accuracy: 0.9699 - val_loss: 0.0840\n",
      "Epoch 8/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9637 - loss: 0.1032 - val_accuracy: 0.9705 - val_loss: 0.0821\n",
      "Epoch 9/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9649 - loss: 0.0975 - val_accuracy: 0.9705 - val_loss: 0.0793\n",
      "Epoch 10/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 1ms/step - accuracy: 0.9662 - loss: 0.0962 - val_accuracy: 0.9707 - val_loss: 0.0812\n",
      "Epoch 11/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9652 - loss: 0.0961 - val_accuracy: 0.9708 - val_loss: 0.0800\n",
      "Epoch 12/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9642 - loss: 0.0963 - val_accuracy: 0.9710 - val_loss: 0.0807\n",
      "Epoch 13/13\n",
      "\u001b[1m3067/3067\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.0937 - val_accuracy: 0.9741 - val_loss: 0.0736\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x276b0c02990>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Input(shape=(vector_size,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "model.fit(train_vectors, y_train,\n",
    "          epochs=13,\n",
    "          batch_size=32,\n",
    "          validation_data=(dev_vectors, y_dev),\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b1b14928",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m171/171\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9746 - loss: 0.0765   \n",
      "Dev Accuracy: 0.9741\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(dev_vectors, y_dev)\n",
    "print(f'Dev Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "c2ca47b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.8899 - loss: 0.2650 - val_accuracy: 0.9608 - val_loss: 0.1051\n",
      "Epoch 2/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9484 - loss: 0.1432 - val_accuracy: 0.9653 - val_loss: 0.0971\n",
      "Epoch 3/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9539 - loss: 0.1297 - val_accuracy: 0.9658 - val_loss: 0.0906\n",
      "Epoch 4/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9539 - loss: 0.1262 - val_accuracy: 0.9615 - val_loss: 0.0961\n",
      "Epoch 5/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9572 - loss: 0.1202 - val_accuracy: 0.9698 - val_loss: 0.0835\n",
      "Epoch 6/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.9605 - loss: 0.1108 - val_accuracy: 0.9689 - val_loss: 0.0828\n",
      "Epoch 7/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9633 - loss: 0.1026 - val_accuracy: 0.9694 - val_loss: 0.0802\n",
      "Epoch 8/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9627 - loss: 0.1020 - val_accuracy: 0.9684 - val_loss: 0.0859\n",
      "Epoch 9/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9639 - loss: 0.1002 - val_accuracy: 0.9690 - val_loss: 0.0811\n",
      "Epoch 10/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9645 - loss: 0.0991 - val_accuracy: 0.9709 - val_loss: 0.0759\n",
      "Epoch 11/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9649 - loss: 0.0974 - val_accuracy: 0.9721 - val_loss: 0.0740\n",
      "Epoch 12/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9665 - loss: 0.0951 - val_accuracy: 0.9720 - val_loss: 0.0720\n",
      "Epoch 13/13\n",
      "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.9678 - loss: 0.0904 - val_accuracy: 0.9730 - val_loss: 0.0712\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x276a09a3990>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_split, X_test_split, y_train_split, y_test_split = train_test_split(\n",
    "    train_vectors, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Input(shape=(vector_size,)))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "model.fit(X_train_split, y_train_split,\n",
    "          epochs=13,\n",
    "          batch_size=32,\n",
    "          validation_data=(X_test_split, y_test_split),\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "4623c2a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m614/614\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 889us/step - accuracy: 0.9734 - loss: 0.0701\n",
      "Test Accuracy: 0.9730\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_test_split, y_test_split)\n",
    "print(f'Test Accuracy: {accuracy:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
